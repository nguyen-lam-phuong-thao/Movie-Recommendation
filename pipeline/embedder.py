"""
Generate sentence embeddings for the cleaned title catalog so the recommender
can serve similarity queries without re-processing text each time.

"""
from __future__ import annotations

import argparse
from pathlib import Path
from typing import Iterable, List

import numpy as np
import pandas as pd

try:
    from sentence_transformers import SentenceTransformer
except ImportError as exc:  
    raise SystemExit() from exc


DATA_DIR = Path(__file__).resolve().parent
DEFAULT_INPUT = DATA_DIR / "artifacts/titles_clean.parquet"
DEFAULT_EMBEDDINGS = DATA_DIR / "artifacts/title_embeddings.npy"
DEFAULT_METADATA = DATA_DIR / "artifacts/titles_metadata.parquet"
DEFAULT_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

METADATA_COLUMNS = [
    "show_id",
    "title",
    "platform",
    "type",
    "release_year",
    # "rating",
    # "duration",
    "country",
    "genre_list",
    "cast",
    "description",
    "search_text",
]

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Encode cleaned title metadata into sentence embeddings."
    )
    parser.add_argument(
        "--input",
        type=Path,
        default=DEFAULT_INPUT,
        help="Path to titles_clean.parquet generated by preprocess.py.",
    )
    parser.add_argument(
        "--embeddings-out",
        type=Path,
        default=DEFAULT_EMBEDDINGS,
        help="Destination .npy file for the dense embeddings.",
    )
    parser.add_argument(
        "--metadata-out",
        type=Path,
        default=DEFAULT_METADATA,
        help="Destination parquet file for the metadata aligned with embeddings.",
    )
    parser.add_argument(
        "--model",
        default=DEFAULT_MODEL,
        help="SentenceTransformer model name or local path.",
    )
    parser.add_argument(
        "--text-column",
        default="search_text",
        help="Column to encode (default: search_text).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="Batch size for model.encode.",
    )
    parser.add_argument(
        "--normalize",
        default=True,
        action=argparse.BooleanOptionalAction,
        help="L2-normalize embeddings for cosine similarity (default: True).",
    )
    return parser.parse_args()


def load_clean_titles(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise SystemExit(f"Input parquet not found: {path}")
    df = pd.read_parquet(path)
    if df.empty:
        raise SystemExit("Input dataset is empty. Run preprocess.py first.")
    return df


def normalize_text(series: pd.Series) -> pd.Series:
    return (
        series.fillna("")
        .astype(str)
        .str.replace(r"\s+", " ", regex=True)
        .str.strip()
    )


def build_corpus(
    df: pd.DataFrame,
    text_column: str,
    fallback_columns: Iterable[str],
) -> List[str]:
    """
    Build the list of texts to embed.

    Primary text source: df[text_column]
    Fallbacks: sequentially fill empty strings with fallback columns.
    """
    if text_column not in df.columns:
        raise SystemExit(f"Column '{text_column}' not found in dataset.")

    corpus = normalize_text(df[text_column])

    for col in fallback_columns:
        if col not in df.columns:
            continue
        fallback = normalize_text(df[col])
        corpus = corpus.mask(corpus.eq(""), fallback)

    # Ensure no NaNs remain
    return corpus.fillna("").tolist()


def texts_validate(texts: List[str]) -> List[str]:
    """
    Final pass to ensure each item is a string.
    """
    cleaned: List[str] = []
    for t in texts:
        if t is None:
            cleaned.append("")
        else:
            cleaned.append(str(t).strip())
    return cleaned


def prepare_metadata(df: pd.DataFrame) -> pd.DataFrame:
    available = [col for col in METADATA_COLUMNS if col in df.columns]
    if not available:
        raise SystemExit("No metadata columns found in dataset.")

    metadata = df[available].copy()
    metadata.insert(0, "vector_id", range(len(metadata)))
    return metadata


def encode_corpus(
    texts: List[str],
    model_name: str,
    batch_size: int,
    normalize: bool,
) -> np.ndarray:
    model = SentenceTransformer(model_name)
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        show_progress_bar=True,
        normalize_embeddings=normalize,
    )
    return embeddings.astype("float32", copy=False)


def save_outputs(
    embeddings: np.ndarray,
    metadata: pd.DataFrame,
    emb_path: Path,
    meta_path: Path,
) -> None:
    emb_path.parent.mkdir(parents=True, exist_ok=True)
    meta_path.parent.mkdir(parents=True, exist_ok=True)

    np.save(emb_path, embeddings)
    metadata.to_parquet(meta_path, index=False)


def main() -> None:
    args = parse_args()

    df = load_clean_titles(args.input)

    # Build corpus from the original dataframe to avoid accidental column drops
    corpus = build_corpus(
        df,
        text_column=args.text_column,
        fallback_columns=["title", "description"],
    )
    corpus = texts_validate(corpus)

    metadata = prepare_metadata(df)

    print(f"Loaded {len(metadata):,} titles. Encoding with {args.model} ...")
    embeddings = encode_corpus(
        texts=corpus,  
        model_name=args.model,
        batch_size=args.batch_size,
        normalize=args.normalize,
    )

    if embeddings.shape[0] != len(metadata):
        raise RuntimeError(
            f"Embeddings count ({embeddings.shape[0]}) does not match metadata rows ({len(metadata)})."
        )

    save_outputs(embeddings, metadata, args.embeddings_out, args.metadata_out)
    print(
        f"Saved embeddings -> {args.embeddings_out} (shape={embeddings.shape})\n"
        f"Saved metadata   -> {args.metadata_out} (rows={len(metadata):,})"
    )


if __name__ == "__main__":
    main()
