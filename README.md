# ğŸ¬ Movie Recommendations (Netflix + Disney+)

The purpose of this **personal project** is to review and strengthen knowledge in **recommendation systems, natural language processing (NLP), and data analysis**.  
To practice an **end-to-end workflow**â€”including **data preprocessing, text embedding, vector-based similarity search**, and **building a backend API and user interface**â€”the project leverages real-world data from streaming platforms (**Netflix** and **Disney+**).

The development process focuses on gaining a deeper understanding of the **design and operation of a content-based recommendation system** by applying widely used tools and techniques.  
Feedback and suggestions for improvement are highly appreciated.

---

## ğŸ“Œ Summary

This project delivers a **content-based recommendation system** for the Netflix and Disney+ catalogs.  
Users can:

- Select titles they like (seed items)
- Apply filters (platform, type, country, release year)
- Receive similar titles based on **sentence embeddings** and **FAISS nearest-neighbor search**

---

## â“ Problem & Solution

### Problem
- Large content catalogs make it difficult to discover relevant new titles.
- Public APIs provide limited support for customized filtering and similarity search.

### Solution
- Clean and normalize raw Netflix + Disney datasets into a unified format.
- Encode title metadata using **`sentence-transformers`** to generate dense embeddings.
- Build a **FAISS index** for fast similarity retrieval.
- Provide a unified **API + UI** that allows users to filter, select seed titles, and fetch recommendations.

---

## ğŸ—ï¸ Architecture & Flow

### 1ï¸âƒ£ Data Pipeline (`pipeline/`)

- **`preprocess.py`**  
  Merges Netflix and Disney CSV files, normalizes text fields, extracts genre/country lists, and builds a unified `search_text`.

- **`embedder.py`**  
  Encodes `search_text` using the `all-MiniLM-L6-v2` model and outputs:
  - `title_embeddings.npy`
  - `titles_metadata.parquet`

- **`indexer.py`**  
  Builds the FAISS index (`titles_faiss.index`) and writes an index manifest.

> All generated outputs are stored in **`artifacts/`** and consumed directly by the backend.

---

### 2ï¸âƒ£ Backend (`backend/`)

- Built with **FastAPI**
- Key endpoints:
  - `GET /api/titles` â€” returns titles matching applied filters
  - `POST /api/recommend` â€” accepts seed IDs + filters and returns recommendations (similarity scores)

- **`recommender_core.py`**  
  Loads embeddings, metadata, and FAISS index; applies filters; queries nearest neighbors; formats responses.

---

### 3ï¸âƒ£ Frontend (`frontend/`)

- Built with **React + TypeScript + Fluent UI**
- Pages:
  - **Home** â€” hero section with filter dialog
  - **Results** â€” grid of filtered titles, seed selection, and recommendation count
  - **Recommendations** â€” final recommendation list

- State management via **Zustand**:
  - `filters`
  - `filteredTitles`
  - `selectedSeedIds`
  - `recommendations`

---

## ğŸ§° Key Technologies

- **Languages**
  - Python 3.12 (pipeline, backend)
  - TypeScript (frontend)

- **Libraries & Frameworks**
  - FastAPI, Uvicorn
  - Pandas, NumPy
  - FAISS
  - sentence-transformers
  - React, Vite, Fluent UI

- **Vector Store**
  - FAISS `IndexFlatIP` (cosine similarity)

- **State Management**
  - Zustand

- **Containerization**
  - `Dockerfile.backend`
  - `frontend/Dockerfile`
  - `docker-compose.yml`

---

## ğŸ—‚ï¸ Data Preparation

### 1ï¸âƒ£ Place raw datasets
```text
Movie_DA/
â”œâ”€â”€ netflix_titles.csv
â””â”€â”€ disney_plus_titles.csv
```
### 2ï¸âƒ£ Run the pipeline

  python pipeline/preprocess.py
  python pipeline/embedder.py
  python pipeline/indexer.py

### 3ï¸âƒ£ Expected outputs (artifacts/)
```text
artifacts/
â”œâ”€â”€ titles_clean.parquet
â”œâ”€â”€ titles_clean.csv
â”œâ”€â”€ title_embeddings.npy
â”œâ”€â”€ titles_metadata.parquet
â”œâ”€â”€ titles_faiss.index
â””â”€â”€ index_manifest.json
```
### â–¶ï¸ Manual Run (Local)
 ## Backend
  ```text
  cd backend
  python -m venv .venv
  source .venv/bin/activate        # Windows: .venv\Scripts\activate
  pip install -r requirements.txt
  uvicorn backend.app:app --reload

  - API: http://127.0.0.1:8000
```
  ## Frontend
  ```text
  cd frontend
  npm install
  npm run dev -- --host 0.0.0.0 --port 5173

  - UI: http://127.0.0.1:5173
  - Ensure VITE_API_BASE_URL points to the backend URL.
```

## User Flow:
Open UI â†’ apply filters â†’ view Results â†’ select seeds â†’ click Recommend â†’ view Recommendations

### ğŸ³ Docker Compose
## Requirements

- artifacts/ must already exist (not version-controlled)

## Configuration

- Backend: Dockerfile.backend
- Frontend: frontend/Dockerfile
- API base URL: http://localhost:8000

## Build & Run
- docker compose up --build

## Access

- Frontend: http://localhost:5173
- API: http://localhost:8000/api/...

### ğŸ“ Project Structure
```text
  Week 6/
  â”œâ”€â”€ backend/
  â”‚   â”œâ”€â”€ app.py
  â”‚   â”œâ”€â”€ recommender_core.py
  â”‚   â”œâ”€â”€ schemas.py
  â”‚   â”œâ”€â”€ recommender.py
  â”‚   â”œâ”€â”€ settings.py
  â”‚   â””â”€â”€ requirements.txt
  â”œâ”€â”€ frontend/
  â”‚   â”œâ”€â”€ src/
  â”‚   â”œâ”€â”€ package.json
  â”‚   â””â”€â”€ Dockerfile
  â”œâ”€â”€ pipeline/
  â”‚   â”œâ”€â”€ preprocess.py
  â”‚   â”œâ”€â”€ embedder.py
  â”‚   â””â”€â”€ indexer.py
  â”œâ”€â”€ artifacts/        # generated outputs (not tracked)
  â”œâ”€â”€ Movie_DA/         # raw datasets (not tracked)
  â”œâ”€â”€ Dockerfile.backend
  â”œâ”€â”€ docker-compose.yml
  â””â”€â”€ .gitignore
```

### ğŸš€ Deployment Notes

- Do not commit Movie_DA/ and artifacts/; provide instructions or download links instead.
- When updating filter schemas, ensure pipeline, backend, and frontend are updated consistently.
- Set VITE_API_BASE_URL to a browser-accessible host.
- For production, consider building the frontend (npm run build) and serving dist/ via the backend or a reverse proxy.
